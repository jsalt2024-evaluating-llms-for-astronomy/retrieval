{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os\n",
    "import glob\n",
    "from topk_sae import FastAutoencoder, loss_fn, unit_norm_decoder_grad_adjustment_, unit_norm_decoder_, init_from_data_\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train(ae, train_loader, optimizer, epochs, k, auxk_coef, clip_grad=None, save_dir=\"checkpoints\", model_name=\"\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    step = 0\n",
    "    num_batches = len(train_loader)\n",
    "    for epoch in range(epochs):\n",
    "        ae.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            x = batch[0].to(device)\n",
    "            recons, info = ae(x)\n",
    "            loss, recons_loss, auxk_loss = loss_fn(ae, x, recons, info, auxk_coef)\n",
    "            loss.backward()\n",
    "            step += 1\n",
    "            \n",
    "            # calculate proportion of dead latents (not fired in last num_batches = 1 epoch)\n",
    "            dead_latents_prop = (ae.stats_last_nonzero > num_batches).float().mean().item()\n",
    "            \n",
    "            unit_norm_decoder_grad_adjustment_(ae)\n",
    "            \n",
    "            if clip_grad is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(ae.parameters(), clip_grad)\n",
    "            \n",
    "            optimizer.step()\n",
    "            unit_norm_decoder_(ae)\n",
    "\n",
    "            topk_indices = torch.cat((info[\"topk_indices\"], info[\"auxk_indices\"]), dim = -1)\n",
    "            selected_grad = torch.abs(ae.encoder.weight.grad[topk_indices, :]).mean()\n",
    "            print('encoder', selected_grad)\n",
    "\n",
    "            mask = torch.ones_like(ae.encoder.weight.grad)\n",
    "            mask[topk_indices, :] = 0\n",
    "            unselected_grad = ae.encoder.weight.grad[mask == 1].mean()\n",
    "            print(unselected_grad)\n",
    "\n",
    "            selected_dgrad = torch.abs(ae.decoder.weight.grad[:, topk_indices]).mean()\n",
    "            print('decoder', selected_dgrad)\n",
    "            unselected_dgrad = ae.decoder.weight.grad[mask.T == 1].mean()\n",
    "            print(unselected_dgrad)\n",
    "            \n",
    "            print(info[\"topk_indices\"], info(\"auxk_indices\"))\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Delete previous model saves for this configuration\n",
    "        for old_model in glob.glob(os.path.join(save_dir, f\"{model_name}_epoch_*.pth\")):\n",
    "            os.remove(old_model)\n",
    "\n",
    "        # Save new model\n",
    "        save_path = os.path.join(save_dir, f\"{model_name}_epoch_{epoch+1}.pth\")\n",
    "        torch.save(ae.state_dict(), save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "\n",
    "d_model = 1536\n",
    "n_dirs = d_model * 6\n",
    "k = 32\n",
    "auxk = 64 #256\n",
    "batch_size = 1024\n",
    "lr = 1e-4\n",
    "epochs = 1\n",
    "auxk_coef = 1/32\n",
    "clip_grad = 1.0\n",
    "\n",
    "# Create model name\n",
    "model_name = f\"{k}_{n_dirs}_{auxk}_final\"\n",
    "\n",
    "data = np.load(\"../data/vector_store/abstract_embeddings.npy\")\n",
    "data_tensor = torch.from_numpy(data).float()\n",
    "dataset = TensorDataset(data_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "ae = FastAutoencoder(n_dirs, d_model, k, auxk).to(device)\n",
    "init_from_data_(ae, data_tensor[:10000].to(device))\n",
    "\n",
    "optimizer = optim.Adam(ae.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/266 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder tensor(2.4457e-06)\n",
      "tensor(0.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 1/266 [00:03<17:34,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder tensor(8.5545e-06)\n",
      "tensor(0.)\n",
      "encoder tensor(2.2086e-06)\n",
      "tensor(0.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|          | 2/266 [00:07<15:19,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder tensor(8.2827e-06)\n",
      "tensor(0.)\n",
      "encoder tensor(2.0640e-06)\n",
      "tensor(0.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|          | 3/266 [00:10<14:13,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder tensor(8.2802e-06)\n",
      "tensor(0.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|          | 3/266 [00:11<17:08,  3.91s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# print abs gradient for topk/auxk and everything else -- encoder and decoder\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauxk_coef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(ae, train_loader, optimizer, epochs, k, auxk_coef, clip_grad, save_dir, model_name)\u001b[0m\n\u001b[1;32m     27\u001b[0m recons, info \u001b[38;5;241m=\u001b[39m ae(x)\n\u001b[1;32m     28\u001b[0m loss, recons_loss, auxk_loss \u001b[38;5;241m=\u001b[39m loss_fn(ae, x, recons, info, auxk_coef)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# calculate proportion of dead latents (not fired in last num_batches = 1 epoch)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/jsalt-retrieval/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jsalt-retrieval/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print abs gradient for topk/auxk and everything else -- encoder and decoder\n",
    "\n",
    "train(ae, train_loader, optimizer, epochs, k, auxk_coef, clip_grad, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autointerp\n",
    "from pathlib import Path\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract_embeddings.npy documents.pkl           keyword_index.json\n",
      "abstract_texts.json     embeddings_matrix.npy   metadata.json\n",
      "document_index.pkl      index_mapping.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/vector_store/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = Path(\"../config.yaml\")\n",
    "DATA_DIR = Path(\"../data\")\n",
    "SAE_DATA_DIR = Path(\"sae_data\")\n",
    "feature_index = 1000\n",
    "num_samples = 5 \n",
    "\n",
    "analyzer = autointerp.NeuronAnalyzer(CONFIG_PATH, feature_index, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1536,)\n",
      "(268065,)\n",
      "[(0, 0.3231629149511315), (1, 0.3884177835290795), (2, 0.37229435697601637), (3, 0.4051744935201531), (4, 0.43787066220692483), (5, 0.40065434325844407), (6, 0.3293621663897055), (7, 0.30941542151710216), (8, 0.46078678381987526), (9, 0.49524984518679194)]\n"
     ]
    }
   ],
   "source": [
    "top_abstracts, zero_abstracts = analyzer.get_feature_activations(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpretation: High-energy neutrino production in cosmic rays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 12%|█▎        | 1/8 [00:02<00:15,  2.18s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 25%|██▌       | 2/8 [00:04<00:14,  2.42s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 25%|██▌       | 2/8 [00:07<00:22,  3.73s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# ground_truth = [1] * (num_samples//divider) + [0] * (num_samples//divider)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(top_abstracts[num_samples\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mdivider:]) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(zero_abstracts[num_samples\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mdivider:])\n\u001b[0;32m---> 15\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterpretation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_abstracts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m correlation, f1 \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39mevaluate_predictions(ground_truth, predictions)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPearson correlation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrelation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/retrieval/saerch/autointerp.py:166\u001b[0m, in \u001b[0;36mNeuronAnalyzer.predict_activations\u001b[0;34m(self, interpretation, abstracts)\u001b[0m\n\u001b[1;32m    161\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPREDICTION_BASE_PROMPT\u001b[38;5;241m.\u001b[39mformat(description\u001b[38;5;241m=\u001b[39minterpretation, abstract\u001b[38;5;241m=\u001b[39mabstract)\n\u001b[1;32m    162\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    163\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#\"gpt-4o\",\u001b[39;00m\n\u001b[1;32m    164\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]\n\u001b[1;32m    165\u001b[0m     )\n\u001b[0;32m--> 166\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPREDICTION:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    167\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(prediction\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "feature_index = 101\n",
    "num_samples = 5\n",
    "\n",
    "analyzer = autointerp.NeuronAnalyzer(CONFIG_PATH, feature_index, num_samples)\n",
    "\n",
    "top_abstracts, zero_abstracts = analyzer.get_feature_activations(num_samples)\n",
    "interpretation = analyzer.generate_interpretation(top_abstracts, zero_abstracts)\n",
    "print(f\"Interpretation: {interpretation}\")\n",
    "\n",
    "divider = 3\n",
    "test_abstracts = [abstract for _, abstract, _ in top_abstracts[num_samples//divider:] + zero_abstracts[num_samples//divider:]]\n",
    "# ground_truth = [1] * (num_samples//divider) + [0] * (num_samples//divider)\n",
    "ground_truth = [1] * len(top_abstracts[num_samples//divider:]) + [0] * len(zero_abstracts[num_samples//divider:])\n",
    "\n",
    "predictions = analyzer.predict_activations(interpretation, test_abstracts)\n",
    "correlation, f1 = analyzer.evaluate_predictions(ground_truth, predictions)\n",
    "\n",
    "print(f\"Pearson correlation: {correlation}\")\n",
    "print(f\"F1 score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{Two separate statistical tests are described and developed in order to test un-binned data sets for adherence to the power-law form. The first test employs the TP-statistic, a function defined to deviate from zero when the sample deviates from the power-law form, regardless of the value of the power index. The second test employs a likelihood ratio test to reject a power-law background in favor of a model signal distribution with a cut-off. }    \\\\begin{document} ',\n",
       " '{\\\\it Astro-H} will be the first X-ray observatory to employ a high-resolution microcalorimeter, capable of measuring the shift and width of individual spectral lines to the precision necessary for estimating the velocity of the diffuse plasma in galaxy clusters. This new capability is expected to bring significant progress in understanding the dynamics, and therefore the physics, of the intracluster medium. However, because this plasma is optically thin, projection effects will be an important complicating factor in interpreting future {\\\\it Astro-H} measurements. To study these effects in detail, we performed an analysis of the velocity field from simulations of a galaxy cluster experiencing gas sloshing, and generated synthetic X-ray spectra, convolved with model {\\\\it Astro-H} Soft X-ray Spectrometer (SXS) responses. We find that the sloshing motions produce velocity signatures that will be observable by {\\\\it Astro-H} in nearby clusters: the shifting of the line centroid produced by the fast-moving cold gas underneath the front surface, and line broadening produced by the smooth variation of this motion along the line of sight. The line shapes arising from inviscid or strongly viscous simulations are very similar, indicating that placing constraints on the gas viscosity from these measurements will be difficult. Our spectroscopic analysis demonstrates that, for adequate exposures, {\\\\it Astro-H} will be able to recover the first two moments of the velocity distribution of these motions accurately, and in some cases multiple velocity components may be discerned. The simulations also confirm the importance of accurate treatment of PSF scattering in the interpretation of {\\\\it Astro-H}/SXS spectra of cluster plasmas. ',\n",
       " 'Silicon oxide thin films play an important role in the realization of optical coatings and high-performance electrical circuits. Estimates of the dielectric function in the far- and mid-infrared regime are derived from the observed transmittance spectrum for a commonly employed low-stress silicon oxide formulation. The experimental, modeling, and numerical methods used to extract the dielectric function are presented. ',\n",
       " '{} {In light of the recent detection of direct evidence for the formation of Kelvin-Helmholtz instabilities in the Orion nebula, we expand upon previous modelling efforts by numerically simulating the shear-flow driven gas and dust dynamics in locations where the H$_{II}$ region and the molecular cloud interact. We aim to directly confront the simulation results with the infrared observations.} {To numerically model the onset and full nonlinear development of the Kelvin-Helmholtz instability we take the setup proposed to interpret the observations, and adjust it to a full 3D hydrodynamical simulation that includes the dynamics of gas as well as dust. A dust grain distribution with sizes between 5-250 nm is used, exploiting the gas+dust module of the MPI-AMRVAC code, in which the dust species are represented by several pressureless dust fluids. The evolution of the model is followed well into the nonlinear phase. The output of these simulations is then used as input for the SKIRT dust radiative transfer code to obtain infrared images at several stages of the evolution, which can be compared to the observations.} {We confirm that a 3D Kelvin-Helmholtz instability is able to develop in the proposed setup, and that the formation of the instability is not inhibited by the addition of dust. Kelvin-Helmholtz billows form at the end of the linear phase, and synthetic observations of the billows show striking similarities to the infrared observations. It is pointed out that the high density dust regions preferentially collect on the flanks of the billows. To get agreement with the observed Kelvin-Helmholtz ripples, the assumed geometry between the background radiation, the billows and the observer is seen to be of critical importance.} {} ',\n",
       " '{\\\\object{PSR~B0031$-$07} is well known to exhibit three different modes of drifting sub-pulses (mode A, B and C). It has recently been shown that in a multifrequency observation, consisting of 2\\\\,700 pulses, all driftmodes were visible at low frequencies, while at 4.85\\\\,GHz only mode-A drift or non-drifting emission was detected. This suggests that modes A and B are emitted in sub-beams, rotating at a fixed distance from the magnetic axis, with the mode-B sub-beams being closer to the magnetic axis than the mode-A sub-beams. Diffuse emission between the sub-beams can account for the non-drifting emission.} {Using the results of an analysis of simultaneous multifrequency observations of \\\\object{PSR~B0031$-$07}, we set out to construct a geometrical model that includes emission from both sub-beams and diffuse emission and describes the regions of the radio emission of \\\\object{PSR~B0031$-$07} at each emission frequency for driftmodes A and B.} {Based on the vertical spacing between driftbands, we have determined the driftmode of each sequence of drift. To restrict the model, we calculated average polarisation and intensity characteristics for each driftmode and at each frequency.} {The model reproduces the observed polarisation and intensity characteristics, suggesting that diffuse emission plays an important role in the emission properties of \\\\object{PSR~B0031$-$07}. The model further suggests that the emission heights of this pulsar range from a few kilometers to a little over 10 kilometers above the pulsar surface. We also find that the relationships between height and frequency of emission that follow from curvature radiation and from plasma-frequency emission could not be used to reproduce the observed frequency dependence of the width of the average intensity profiles.} {} ',\n",
       " 'We have used infrared polarimetric imaging with NICMOS to determine precisely the position of the star that illuminates (and presumably generated) the bipolar, pre-planetary reflection nebula RAFGL 2688 (the Egg Nebula). The polarimetric data pinpoint the illuminating star, which is not detected directly at wavelengths $\\\\le$ 2 $\\\\mu$m, at a position well within the dark lane that bisects the nebula, 0\\\\farcs55 ($\\\\sim550$ AU) southwest of the infrared peak which was previously detected at the southern tip of the northern polar lobe. The inferred position of the central star corresponds to the geometric center of the tips of the four principle lobes of near-infrared H$_2$ emission; identifying the central star at this position also reveals the strong point symmetric structure of the nebula, as seen both in the intensity and polarization structure of the polar lobes. The polarimetric and imaging data indicate that the infrared peak directly detected in the NICMOS images is a self-luminous source and, therefore, is most likely a distant binary companion to the illuminating star. Although present theory predicts that bipolar structure in pre-planetary and planetary nebulae is a consequence of binary star evolution, the separation between the components of the RAFGL 2688 binary system, as deduced from these observations, is much too large for the presence of the infrared companion to have influenced the structure of the RAFGL 2688 nebula. ',\n",
       " \"We analyze the structural parameters of the largest-available sample of spatially resolved extragalactic globular clusters. The images of M31 GCs were found in a search of HST archival data, described in a companion paper. We measure the ellipticities and position angles of the clusters and conclude that the ellipticities are consistent with being caused by rotation. We find that most clusters' surface brightness distributions are well-fit by two-dimensional single-mass Michie-King models. A few clusters show possible power-law distributions characteristic of core-collapse, but the spatial resolution is not high enough to make definitive claims. As has been found for other galaxies, the metal-rich clusters are slightly smaller than the metal-poor clusters. There are strong correlations between structural properties of M31 GCs, as for Milky Way clusters, and the two populations are located close to the same `fundamental plane' in parameter space. \",\n",
       " 'We discuss the basic hydrodynamics that determines the density structure of the disks around hot stars.  Observational evidence supports the idea that these disks are Keplerian (rotationally supported) gaseous disks.  A popular scenario in the literature, which naturally leads to the formation of Keplerian disks, is the viscous decretion model. According to this scenario, the disks are hydrostatically supported in the vertical direction, while the radial structure is governed by the viscous transport. This suggests that the temperature is one primary factor that governs the disk density structure. In a previous study we demonstrated, using 3-D NLTE Monte Carlo simulations, that viscous keplerian disks can be highly non-isothermal. In this paper we build upon our previous work and solve the full problem of the steady-state non-isothermal viscous diffusion and vertical hydrostatic equilibrium. We find that the self-consistent solution departs significantly from the analytic isothermal density, with potentially large effects on the emergent spectrum. This implies that non-isothermal disk models must be used for a detailed modeling of Be star disks. ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3, 0.9, -0.2, -0.8, -0.8, -0.7, -0.8, -0.8]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jsalt-retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
